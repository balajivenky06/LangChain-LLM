{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIVTlfviLK03"
   },
   "source": [
    "# Introduction to LLM using LangChain\n",
    "# Building a Language Model Application: LLMs\n",
    "- https://python.langchain.com/en/latest/getting_started/getting_started.html\n",
    "- This quickstart guide provides you a quick walkthrough about building an end-to-end language model application with LangChain\n",
    "- This helps you to know high level understanding of what the framework is capable of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2wSBdVwMEHm"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQyILM-6Lufj",
    "outputId": "46c66f30-8841-498f-e35a-3bec260aaf72"
   },
   "outputs": [],
   "source": [
    "#!pip install langchain openai huggingface_hub watermark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSCVHcEYS7FA",
    "outputId": "c1262455-549d-4c74-c0f7-7937168f9677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Balaji Venktesh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Balaji Venktesh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWSS4cN7M3Zw"
   },
   "source": [
    "## Environment Setup\n",
    "- Using LangChain will usually require integrations with one or more model providers, data stores, apis, etc.\n",
    "- <font color=\"red\">HuggingfaceHub API (its free)</font>\n",
    "- <font color=\"red\">OpenAI's API (its not free)</font>\n",
    "\n",
    "**Get api keys (you need to create account for both to access the api keys)**\n",
    "\n",
    "- Get OpenAI api key: https://platform.openai.com/account/api-keys\n",
    "- Huggingface api key: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W19xmh93Pr45"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"HUGGINGFACEHUB_API_TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZVZuC0VOVm6"
   },
   "source": [
    "üìùNOTE: LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmsfPQxgO32D"
   },
   "source": [
    "## LLMs: Get Prediction from a language model\n",
    "- This is the most basic building block of LangChain.\n",
    "- Provides a uniform interface to interact with different large langauge models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2qoYayO4Mk3Y"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenAI??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5JsKY0EUzFV"
   },
   "source": [
    "- The temperature argument in the OpenAI LLM wrapper is used to <font color=\"red\">control the level of randomness in the generated text.</font>\n",
    "- <font color=\"cyan\">A higher temperature value will result in more diverse and unpredictable text, while a lower temperature value will result in more conservative and predictable text. </font>\n",
    "- The default value for temperature is 1.0, and valid values range from 0.0 to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1hIkwQ3QG-_",
    "outputId": "5e012f00-c3b4-49a6-af4b-e6fc64576f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as well. A few of the most important ones were\n",
      "\n",
      "- BERT\n",
      "- GPT-3\n",
      "- DistilBERT\n",
      "- RoBERTa\n",
      "- ALBERT\n",
      "- XLNet\n",
      "\n",
      "All of these models have their own properties and advantages depending on the task they are used for. BERT is one of the most widely used models and is known for its ability to improve performance on a variety of Natural Language Processing tasks. GPT-3 is a powerful open-source language model that is capable of understanding and responding to natural language queries. DistilBERT is a smaller, faster version of BERT that is capable of achieving similar performance as the original model. RoBERTa is an improved version of BERT that has multiple tweaks to optimize performance and is more accurate on a variety of NLP tasks. ALBERT is an improved version of BERT that is lighter and faster while still being able to achieve similar performance as the original model. XLNet is a modern language model that outperforms BERT on certain NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "# initialize the wrapper around OpenaI LLMs and call it on some input\n",
    "llm = OpenAI(temperature=0.9) # model_name=\"text-davinci-003\"\n",
    "text = \"Tried experimenting with Open API LLM Models\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coLUy0KKZb7Q"
   },
   "source": [
    "- The default Hugging Face Hub inference APIs do not use specialized hardware. They are also not suitable for running larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl`)\n",
    "- You can try changing the runtime in google colab to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BGkpjmP0WLch"
   },
   "outputs": [],
   "source": [
    "#HuggingFaceHub??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52qx9JY6Tqc6",
    "outputId": "719f9432-ef80-44ea-ea66-d66ba8948eb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\welcome\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# initialize the wrapper around HuggingfaceHub models and call it on some input\n",
    "hub_llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0.9, \"max_length\":64}) # model_name=\"text-davinci-003\"\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(hub_llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6kIwuIoCQo7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aYG3masavjn"
   },
   "source": [
    "## Prompt Templates: Mange prompts for LLMs\n",
    "- <font color=\"orange\">Instead of hard-coding thet text we want to aks, we can use prompt template to manage the prompt.</font>\n",
    "- Also, when creating application, its not feasible to pass the input directly to LLM.\n",
    "- It's similar to `python f-strings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gV4ZjNhNosGH",
    "outputId": "83f28afc-5703-4402-873d-48678f4a19d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm learning LangChain.\n"
     ]
    }
   ],
   "source": [
    "# python f-string example\n",
    "framework = \"LangChain\"\n",
    "print(f\"I'm learning {framework}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hZSJiNadW1EP"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yux9wGENbXM5",
    "outputId": "cf414736-b3f6-43e5-c1e9-eec1dec56117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes colorful socks?\n"
     ]
    }
   ],
   "source": [
    "# now just calling the `.format` method will format it.\n",
    "print(prompt.format(product=\"colorful socks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AB0MsxMCRYZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u8hfDLuqO80"
   },
   "source": [
    "## Chains: Combine LLMs and Prompts in multi-step workflows\n",
    "- In real application, Using LLM in isolation is OK for some applications but in most of the cases it requires chaining. And chaining with PromptTemplate might be a neccessity.\n",
    "- A chain in LangChain is made up of links, which can be either primitives like LLMs or other chains\n",
    "- <font color=\"red\">Extending the previous example, we can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ruwNQohttvJ7"
   },
   "outputs": [],
   "source": [
    "#PromptTemplate??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "atpILVEPbpDQ"
   },
   "outputs": [],
   "source": [
    "#hub_llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0.9, \"max_length\":64}) # model_name=\"text-davinci-003\"\n",
    "llm = OpenAI(temperature=0.9) # model_name=\"text-davinci-003\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WbdogSW2uEZR"
   },
   "outputs": [],
   "source": [
    "#create a simple chain that takes user input, format the prompt with it and send it to LLM\n",
    "from langchain.chains import LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yE2Mhr_2ukqO",
    "outputId": "e4f97116-80a7-411e-ef59-8d93afd4728a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SockFiesta\n"
     ]
    }
   ],
   "source": [
    "# we can now run the chain only specifying the product\n",
    "#chain.run(\"colorful socks\")\n",
    "print(llm_chain.run(\"colorful socks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-k_l0l8wCSNy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQBWXNR4z-cz"
   },
   "source": [
    "## Agents: Dynamically call chains based on user input\n",
    "- So, far what we did was to run the chains in a predetermined order.\n",
    "- <font color =\"orange\"> Agents can use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output or returning to the user.</font>\n",
    "\n",
    "In order to load agents, understanding the following concepts is crucial.\n",
    "\n",
    "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
    "\n",
    "- LLM: The language model powering the agent.\n",
    "\n",
    "- Agents: Agents involve an LLM making decisions about which actions to take, taking that Action, seeing an observation, and repeating that until its done.\n",
    "\n",
    "Agents: For a list of supported agents and their specifications, see [here](https://python.langchain.com/en/latest/modules/agents/agents.html).\n",
    "\n",
    "Tools: For a list of predefined tools and their specifications, see [here](https://python.langchain.com/en/latest/modules/agents/tools.html).\n",
    "\n",
    "<font color=\"red\">For this example, you will also need to install the [SerpAPI Python package](https://pypi.org/project/google-search-results/).</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axoJAs402uQS",
    "outputId": "362d78fb-4c1a-4ee5-a65d-6da89624b87a"
   },
   "outputs": [],
   "source": [
    "!pip install google-search-results --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ihxWArPc4L_O"
   },
   "outputs": [],
   "source": [
    "# https://serpapi.com/\n",
    "import os\n",
    "os.environ['SERPAPI_API_KEY'] = \"SERAPI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "FdiZ7Ueg6fBv",
    "outputId": "9ee7754e-79ee-44ec-84fb-d45adb0e536f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should research this to find out more information.\n",
      "Action: Search\n",
      "Action Input: \"LangChain\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[\"LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\", 'LangChain kgmid: /g/11kjpl7_60.', 'LangChain initial_release_date: October 2022.', 'LangChain developer_s: Harrison Chase.', 'LangChain license: MIT License.', 'LangChain stable_release: 0.0.320 / 21 October 2023; 20 days ago.', 'LangChain written_in: Python and JavaScript.', \"LangChain's flexible abstractions and extensive toolkit unlocks developers to build context-aware, reasoning LLM applications.\", 'LangChain is a framework for developing applications powered by language models. It enables applications that: Are context-aware: connect a language model to ...', 'LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, ...', \"LangChain uses the power of AI large language models combined with data sources to create quite powerful apps. Read how it works and how it's used.\", \"Morningstar Intelligence Engine puts personalized investment insights at analyst's fingertips ¬∑ 2 min read ; Parallel Function Calling for Structured Data ...\", 'LangChain for LLM Application Development is a beginner-friendly course. Basic Python knowledge will help you get the most out of this course.', 'Memory refers to persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m From the search results, it looks like LangChain is a framework for developing applications powered by language models.\n",
      "Final Answer: LangChain is a framework for developing applications powered by language models. It enables applications that are context-aware and use the power of AI large language models combined with data sources to create quite powerful apps.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain is a framework for developing applications powered by language models. It enables applications that are context-aware and use the power of AI large language models combined with data sources to create quite powerful apps.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "#tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "tools = load_tools([\"serpapi\"])\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"What is LangChain ?\")\n",
    "#agent.run(\"What was the high temperature in Helsinki yesterday in Celsius? What is that number in Fahrenheit ?\")\n",
    "#agent.run(\"Who is the president of Finland ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Bmy4qnPCTYB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqj9WHbK-VGp"
   },
   "source": [
    "## Memory: Add State to Chains and Agents\n",
    "- So far, all the chains and agents we‚Äôve gone through have been stateless. But often, you may want a chain or agent to have some concept of ‚Äúmemory‚Äù so that it may remember information about its previous interactions.\n",
    "- For example, while designing a chatbot you want it to remember previous message or previous several messages.\n",
    "- Short-term memory\n",
    "- Long-term-memory (remembering key pieces of information over time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kAnoBgSv7SVC",
    "outputId": "9013ab4c-1da6-4206-db7a-a90d1ad07530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! It's nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAT19mCV-8u2",
    "outputId": "973a2b1c-a907-4d11-cfbf-bcbabb41e8af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3iNCgxm_A4R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
