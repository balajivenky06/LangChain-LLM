{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIVTlfviLK03"
   },
   "source": [
    "# Quickstart Guide (part 2)\n",
    "# Building a Language Model Application: Chat Models\n",
    "- https://python.langchain.com/en/latest/getting_started/getting_started.html\n",
    "- This 2nd part of quickstart guide provides you a quick walkthrough about using chat models instead of LLMs.\n",
    "- Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a ‚Äútext in, text out‚Äù API, they expose an interface where ‚Äúchat messages‚Äù are the inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2wSBdVwMEHm"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WQyILM-6Lufj"
   },
   "outputs": [],
   "source": [
    "!pip install langchain openai huggingface_hub watermark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xSCVHcEYS7FA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Balaji Venktesh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Balaji Venktesh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWSS4cN7M3Zw"
   },
   "source": [
    "## Environment Setup\n",
    "- Using LangChain will usually require integrations with one or more model providers, data stores, apis, etc.\n",
    "\n",
    "- Good to know: <font color=\"red\">OpenAI's API (its not free)</font>\n",
    "\n",
    "**Get api keys (you need to create account for both to access the api keys)**\n",
    "\n",
    "- Get OpenAI api key: https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W19xmh93Pr45"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZVZuC0VOVm6"
   },
   "source": [
    "üìùNOTE: LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmsfPQxgO32D"
   },
   "source": [
    "## Get Message Completions from a Chat Model\n",
    "- We can get chat completions by passing one or more messages to chat model and response will be message.\n",
    "- <font color=\"orange\"> The types of messages currently supported in LangChain are `AIMessage`, `HumanMessage`, `SystemMessage` and `ChatMessage` ‚Äì `ChatMessage` takes in an arbitrary role parameter. Most of the time, you‚Äôll just be dealing with `HumanMessage`, `AIMessage`, and `SystemMessage`. </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2qoYayO4Mk3Y"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5JsKY0EUzFV"
   },
   "source": [
    "- The temperature argument in the OpenAI LLM wrapper is used to <font color=\"red\">control the level of randomness in the generated text.</font>\n",
    "- <font color=\"cyan\">A higher temperature value will result in more diverse and unpredictable text, while a lower temperature value will result in more conservative and predictable text. </font>\n",
    "- The default value for temperature is 1.0, and valid values range from 0.0 to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1hIkwQ3QG-_",
    "outputId": "837fd35f-3954-442a-d7eb-5bcee1c660a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='‡Æé‡Æ©‡Æï‡Øç‡Æï‡ØÅ ‡Æ™‡ØÅ‡Æ∞‡Øã‡Æï‡Æø‡Æ∞‡Ææ‡ÆÆ‡Æø‡Æô‡Øç ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡Æï‡Øã‡Æ≥‡Øç. (Enakku programming vƒì·πá·π≠uk≈ç·∏∑)')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now get completions by passing in a single message\n",
    "chat([HumanMessage(content=\"Translate this sentence from English to Tamil. I love programming.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-SSkxfUmEO6h",
    "outputId": "4a14bd1e-3234-440a-c930-1a817dce8e3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Æø‡Æ∞‡Øã‡Æï‡Øç‡Æ∞‡Ææ‡ÆÆ‡Æø‡Æô‡Øç ‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡Æï‡Øç‡Æï‡ØÅ‡Æ§‡Øç ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ™‡Øç‡Æ™‡Æø‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. (NƒÅ·πâ pr≈çgrƒÅmi·πÖ arumaikku tiruppikire·πâ)')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can pass multiple messages for OpenAI‚Äôs gpt-3.5-turbo and gpt-4 models\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to Tamil.\"),\n",
    "    HumanMessage(content=\"Translate this sentence from English to Tamil. I love programming.\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkx4vferEywv"
   },
   "source": [
    "You can go one step further and generate completions for multiple sets of messages using `generate`. This returns an `LLMResult` with an additional `message` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mABmBfbWFEJi",
    "outputId": "1566e64d-8b3c-4cab-a87d-dc60cc1a96cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Æø‡Æ∞‡Øã‡Æï‡Øç‡Æ∞‡Ææ‡ÆÆ‡Æø‡Æô‡Øç ‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡Æï‡Øç‡Æï‡ØÅ‡Æ§‡Øç ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ™‡Øç‡Æ™‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. (NƒÅ·πâ pir≈çgrƒÅmi·πÖ arumaikkit tiruppikki·πüƒì·πâ.)', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Æø‡Æ∞‡Øã‡Æï‡Øç‡Æ∞‡Ææ‡ÆÆ‡Æø‡Æô‡Øç ‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡Æï‡Øç‡Æï‡ØÅ‡Æ§‡Øç ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ™‡Øç‡Æ™‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. (NƒÅ·πâ pir≈çgrƒÅmi·πÖ arumaikkit tiruppikki·πüƒì·πâ.)'))], [ChatGeneration(text='‡Æ®‡Ææ‡Æ©‡Øç ‡ÆÆ‡ØÜ‡ÆØ‡Øç‡Æ™‡Øç‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Øç ‡ÆÆ‡Ææ‡Æ©‡Æø‡ÆØ‡ÆÆ‡Øç ‡ÆÖ‡Æ©‡ØÅ‡Æ™‡Æµ‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. (NƒÅ·πâ meypporu·∏∑ mƒÅ·πâiyam anupavikki·πüƒì·πâ.)', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='‡Æ®‡Ææ‡Æ©‡Øç ‡ÆÆ‡ØÜ‡ÆØ‡Øç‡Æ™‡Øç‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Øç ‡ÆÆ‡Ææ‡Æ©‡Æø‡ÆØ‡ÆÆ‡Øç ‡ÆÖ‡Æ©‡ØÅ‡Æ™‡Æµ‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. (NƒÅ·πâ meypporu·∏∑ mƒÅ·πâiyam anupavikki·πüƒì·πâ.)'))]], llm_output={'token_usage': {'completion_tokens': 180, 'prompt_tokens': 69, 'total_tokens': 249}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('2d739503-d3e4-4296-8771-4289916e390e')), RunInfo(run_id=UUID('5d230759-a692-4b14-b915-9665af7c9361'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Tamil.\"),\n",
    "        HumanMessage(content=\"Translate this sentence from English to Tamil. I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Tamil.\"),\n",
    "        HumanMessage(content=\"Translate this sentence from English to Tamil. I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kbZKbCE0Ex6r",
    "outputId": "0e016178-2253-4f20-86fc-0051aaea7350"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_tokens': 180, 'prompt_tokens': 69, 'total_tokens': 249}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from the output we get we can even extract specific things like token usage\n",
    "result.llm_output['token_usage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aYG3masavjn"
   },
   "source": [
    "## Chat Prompt Templates\n",
    "- <font color=\"orange\">Instead of hard-coding thet text we want to ask, similar to LLMs, we can use make use of templating by using a `MessagePromptTemplate`.</font>\n",
    "- We can build a `ChatPromptTemplate` from one or more `MessagePromptTemplate`s.\n",
    "- We can use `ChatPromptTemplate`‚Äôs `format_prompt` ‚Äì this returns a `PromptValue`, which we can convert to a string or `Message` object, depending on whether you want to use the formatted value as input to an llm or chat model.\n",
    "- For simplicity, there is a `from_template` method exposed on the template which makes our task lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZSJiNadW1EP",
    "outputId": "a977e400-f21f-4b77-bbd8-f1ef8595647e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÅ‡Æ∞‡Æ£‡Øç‡Æü ‡Æ®‡Æø‡Æ∞‡Æ≤‡Ææ‡Æï‡Øç‡Æï‡ÆÆ‡Øç ‡ÆÖ‡Æ§‡Æø‡Æï‡ÆÆ‡Øç ‡Æ™‡Æø‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. (NƒÅ·πâ pu·πüa·πá·π≠a niralaakkam adhikam pi·π≠ikki·πüƒì·πâ.)')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"Tamil\", text=\"I love programming.\").to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u8hfDLuqO80"
   },
   "source": [
    "## Chains and Chat Models\n",
    "- In real application, Using LLM in isolation is OK for some applications but in most of the cases it requires chaining. And chaining with PromptTemplate might be a neccessity.\n",
    "- A chain in LangChain is made up of links, which can be either primitives like LLMs or other chains\n",
    "- <font color=\"red\">Similar to what we did for LLMs in the previous video, extending the previous example, we can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ruwNQohttvJ7"
   },
   "outputs": [],
   "source": [
    "#ChatPromptTemplate??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "atpILVEPbpDQ",
    "outputId": "cd7a9075-9014-455b-b25b-abfd53e408d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÅ‡Æ∞‡Æ£‡Øç‡Æü ‡Æ®‡Æø‡Æ∞‡Æ≤‡Ææ‡Æï‡Øç‡Æï‡ÆÆ‡Øç ‡ÆÖ‡Æ§‡Æø‡Æï‡ÆÆ‡Øç ‡Æ™‡Æø‡Æü‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. (NƒÅ·πâ pu·πüa·πá·π≠a niralaakkam adhikam pi·π≠ikki·πüƒì·πâ.)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"Tamil\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQBWXNR4z-cz"
   },
   "source": [
    "## Agents and Chat Models\n",
    "- So, far what we did was to run the chains in a predetermined order.\n",
    "- <font color =\"orange\"> Agents can also be used with chat models, you can initialize one using `AgentType`. `CHAT_ZERO_SHOT_REACT_DESCRIPTION` as the agent type.</font>\n",
    "\n",
    "In order to load agents, understanding the following concepts is crucial.\n",
    "\n",
    "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
    "\n",
    "- Chat Models: The chat models powering the agent.\n",
    "\n",
    "- Agents: Agents involve an LLM making decisions about which actions to take, taking that Action, seeing an observation, and repeating that until its done.\n",
    "\n",
    "Agents: For a list of supported agents and their specifications, see [here](https://python.langchain.com/en/latest/modules/agents/agents.html).\n",
    "\n",
    "Tools: For a list of predefined tools and their specifications, see [here](https://python.langchain.com/en/latest/modules/agents/tools.html).\n",
    "\n",
    "<font color=\"red\">For this example, you will also need to install the [SerpAPI Python package](https://pypi.org/project/google-search-results/).</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axoJAs402uQS"
   },
   "outputs": [],
   "source": [
    "!pip install google-search-results --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ihxWArPc4L_O"
   },
   "outputs": [],
   "source": [
    "# https://serpapi.com/\n",
    "import os\n",
    "#os.environ['SERPAPI_API_KEY'] = \"SERPAPI_API_KEY\"\n",
    "os.environ['SERPAPI_API_KEY'] = \"f3257c47ef2f9289ee834b6585ceadfc880da3c9331f8e242c7ee6612861c385\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo8e2qIMJ90t"
   },
   "source": [
    "LLM Math is a Python package that showcases using LLMs and Python REPLs to do complex word math problems. It allows you to input a math problem in natural language and get the answer in return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "FdiZ7Ueg6fBv",
    "outputId": "72510136-af97-46a0-cff5-493df7591c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: Who is the president of Finland?\n",
      "Thought: I can search for the current president of Finland.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"current president of Finland\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSauli Niinist√∂\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe current president of Finland is Sauli Niinist√∂.\n",
      "Question: What is 2+2?\n",
      "Thought: I can use a calculator to find the sum of 2+2.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"2+2\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 4\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe sum of 2+2 is 4.\n",
      "Final Answer: The current president of Finland is Sauli Niinist√∂. The sum of 2+2 is 4.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current president of Finland is Sauli Niinist√∂. The sum of 2+2 is 4.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"Who is the president of Finland ? What is 2+2 ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqj9WHbK-VGp"
   },
   "source": [
    "## Memory: Add State to Chains and Agents\n",
    "- So far, all the chains and agents we‚Äôve gone through have been stateless. But often, you may want a chain or agent to have some concept of ‚Äúmemory‚Äù so that it may remember information about its previous interactions.\n",
    "- For example, while designing a chatbot you want it to remember previous message or previous several messages.\n",
    "- Short-term memory\n",
    "- Long-term-memory (remembering key pieces of information over time)\n",
    "- <font color=\"red\"> You can use Memory with chains and agents initialized with chat models. The main difference between this and Memory for LLMs is that rather than trying to condense all previous messages into a string, we can keep them as their own unique memory object.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "kAnoBgSv7SVC",
    "outputId": "b1cd1456-1ca2-435e-cbaa-de630c28552e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "m3iNCgxm_A4R",
    "outputId": "f87041fa-1fae-4457-face-26854f7f171f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great to hear! I'm here to chat and answer any questions you may have. What's on your mind?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "6C519CjyMEBG",
    "outputId": "144580d5-d8c6-4cba-ca76-63812dbc564a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! I am an AI language model developed by OpenAI called GPT-3. I have been trained on a vast amount of text data from the internet, which allows me to generate human-like responses to a wide range of queries and engage in conversations. My purpose is to assist and provide information to the best of my abilities. Is there anything specific you would like to know about me?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lkm2v2kDMvnF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcEQG_NyPC9N"
   },
   "source": [
    "## Conclusion\n",
    "- <font color=\"red\">The decision to use a chat model or an LLM would depend on the specific task you are trying to accomplish. Chat models are designed to have structured conversations with humans, so they would be useful for tasks like creating chatbots or customer service agents. On the other hand, LLMs are more general language models that can be used for a wide range of tasks, such as language translation, text generation, or summarization.</font>\n",
    "\n",
    "- <font color=\"orange\">In general, if the task involves interacting with humans in a conversational way, a chat model would be the better choice. But if the task involves generating or processing large amounts of text, an LLM would be more appropriate. It's also worth noting that LLMs can be used in conjunction with other models, such as text embedding models, to improve their performance on specific tasks.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PR0-gVG9PBwj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
